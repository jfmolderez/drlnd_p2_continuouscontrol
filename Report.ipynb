{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control - Report\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "The algorithm is DDPG (Deep Deterministic Policy Gradient). \n",
    "DDPG is \n",
    "\n",
    "- model free : there is no model of the environment; the agent uses trial and error learning to produce a policy\n",
    "- off policy : the policy is not learnt from the current interactions with the environment but from samples of past interactions\n",
    "The DDPG agent is equipped with an actor network (with local and target version) and a critic network (with local and target version). The actor network learns the optimal policy deterministically. The optimal policy maps any given state to the best believed action for this state :\n",
    "$$ s \\rightarrow \\mu(s ; \\theta_{\\mu}) $$\n",
    "The critic network approximates the Q-value function that maps a state and to its value :\n",
    "$$ s \\rightarrow Q(s, \\mu(s ; \\theta_{\\mu}); \\theta_{Q}) $$\n",
    "\n",
    "The agent runs episodes of interactions with the environment and learns periodically from its past interactions :\n",
    "At each step, the agent :\n",
    "> determines the action a from the current state $ a = \\mu_{local}(s; \\theta_{\\mu_{local}}) $ <br>\n",
    "> interacts with the environment <br>\n",
    "> stores the interaction $<s, a, r, s'>$ in its replay buffer <br>\n",
    "> after a given number of steps, the agent samples a batch of interactions and updates its actor and critic networks along with a soft update of the target networks <br><br>\n",
    "\n",
    "> updates the critic network (for a sample of interactions $<s, a, r, s'>$):\n",
    ">>$a' = \\mu_{target}(s; \\theta_{\\mu_{target}})$<br>\n",
    ">>$\\hat{q} = r + \\gamma * Q_{target}(s', a'; \\theta_{Q_{target}})$ <br>\n",
    ">>$q = Q_{local}(s, a, \\theta_{Q_{local}})$ <br>\n",
    ">> Minimize $Loss(\\hat(q), q)$ <br>\n",
    ">> Update $\\theta_{Q_{local}}$ <br>\n",
    "\n",
    "> updates the actor network (for a sample of interactions $<s, a, r, s'>$):\n",
    ">> $\\tilde{a} = \\mu_{local}(s; \\theta_{\\mu_{local}})$<br>\n",
    ">> $\\tilde{q} = Q_{local}(s, \\tilde{a}; \\theta_{Q_{local}}))$<br> \n",
    ">> Maximize $Mean(\\tilde{q})$<br>\n",
    ">> Update $\\theta_{\\mu_{local}}$ <br>\n",
    "\n",
    ">perform soft update of the target networks :\n",
    ">> $ \\theta_{\\mu_{target}} = \\tau * \\theta_{\\mu_{local}} + (1 - \\tau) * \\theta_{\\mu_{target}}$ <br>\n",
    ">> $ \\theta_{Q_{target}} = \\tau * \\theta_{Q_{local}} + (1 - \\tau) * \\theta_{Q_{target}}$<br>\n",
    "\n",
    "The target networks are used by the critic to produce a prediction of the qvalue $\\hat{q}$ while the local networks are used to produce the estimates of the qvalue of current state and action $q$ and estimates of best action $\\tilde{a}$ and corresponding qvalue $\\tilde{q}$. Only the parameters of the local networks ($\\theta_{\\mu_{local}}$ , $\\theta_{Q_{local}}$) are learnt. The parameters of the target networks follow those of the local networks using soft updates.\n",
    "   \n",
    "\n",
    "### Implementation\n",
    "- Environment : Unity environment provided (Reacher.app)\n",
    "- Agent : DDPG, equipped with actor networks (local and target) critic networks (local and targets) and a replay buffer to store the interactions\n",
    "- The ddpg function drives the agent to interact with the environment through episodes and to learn.\n",
    "- A checkpoint file for the agent local actor network is saved and can be used to test the agent after it has been trained.\n",
    "\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5) # Replay buffer size\n",
    "BATCH_SIZE = 512       # mini batch size\n",
    "GAMMA = 0.99           # discount factor\n",
    "TAU = 1e-3             # for soft update of the target parameters\n",
    "LR_ACTOR = 1e-4        # Learning rate of actor\n",
    "LR_CRITIC = 1e-3       # Learning rate of critic\n",
    "UPDATE_EVERY = 4       # Periodicity : learn & soft updates every 4 interactions\n",
    "```\n",
    "\n",
    "### Results\n",
    "![](results.png)\n",
    "\n",
    "\n",
    "\n",
    "### Further Research\n",
    "- Tuning hyperparameters : in particular the periodicity of learning and the batch size\n",
    "- Improving the architectures of the networks : adding an extra layer to the actor networks\n",
    "- Trying another algorithm : PPO but taking in account the continuous action spaces; the networks will produce statistics ($\\mu$, $\\sigma$) for the distributions or actions and the actions will then be sampled from these distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
